\documentclass[twocolumn]{extarticle}
\usepackage{fontspec}   %加這個就可以設定字體
\usepackage{xeCJK}       %讓中英文字體分開設置
\usepackage{indentfirst}
\usepackage{listings}
\usepackage[newfloat]{minted}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{titling}

\usepackage{pdftexcmds}
\usepackage{catchfile}
\usepackage{ifluatex}
\usepackage{ifplatform}
\usepackage{easyReview}

\usepackage[breakable, listings, skins, minted]{tcolorbox}
\usepackage[htt]{hyphenat}
\usepackage[title]{appendix}

\setlength{\columnsep}{2em}


\usepackage{etoolbox}
\setminted{fontsize=\footnotesize}
\renewtcblisting{minted}{%
	listing engine=minted,
	minted language=python,
	listing only,
	breakable,
	enhanced,
	minted options = {
		linenos, 
		breaklines=true, 
		breakbefore=., 
		fontsize=\footnotesize, 
		numbersep=2mm
	},
	overlay={%
		\begin{tcbclipinterior}
			\fill[gray!25] (frame.south west) rectangle ([xshift=4mm]frame.north west);
		\end{tcbclipinterior}
	}   
}

\usepackage[
top=1.5cm,
bottom=1.5cm,
left=1.5cm,
right=1.5cm,
includehead,includefoot,
heightrounded, % to avoid spurious underfull messages
]{geometry} 

\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Code}
\usepackage[moderate]{savetrees}


\title{AI Capstone: Project 1 Report}
\author{110550088 李杰穎}
\date{\today}


\setCJKmainfont{Noto Serif TC}

\iflinux
\setmonofont[Mapping=tex-text]{Cascadia Code}
\fi

\ifwindows
\setmonofont[Mapping=tex-text]{Consolas}
\fi

\XeTeXlinebreaklocale "zh"             %這兩行一定要加，中文才能自動換行
\XeTeXlinebreakskip = 0pt plus 1pt     %這兩行一定要加，中文才能自動換行

% \setlength{\parindent}{0em}
\setlength{\parskip}{0.75em}
\renewcommand{\baselinestretch}{1}
\setlength{\droptitle}{-8.5em}   % This is your set screw

\begin{document}

\maketitle

\section{Public Image Datasets: CIFAR-10}
\subsection{Datasets Description}

CIFAR-10 is a popular image classification datasets that is frequently used as a benchmark for computer vision and deep learning tasks. It contains 60,000 32x32 color images of 10 different objects, with 6,000 images per class. The classes include airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The datasets is divided into 50,000 training images and 10,000 testing images. CIFAR-10 is challenging due to the small image size and the high variability of the objects within each class. It is often used for developing and testing new machine learning algorithms and architectures for image classification.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figure/cifar}
\caption{Ten randomly select images from 10 classes.}
\label{fig:cifar}
\end{figure}

In this project, I will directly use the datasets provided in the \texttt{torchvision} library, which is part of the PyTorch deep learning framework. Torchvision is a library consists of lots of datasets (including MNIST, CIFAR-10, ImageNet, ...) and pretrained model (including ResNet, VGGNet, BERT, ...).

As for the test set, I will use the original train/test split of the CIFAR-10 datasets. The datasets is divided into 50,000 training images and 10,000 testing images, with an equal number of images from each class in both sets. This train/test split is commonly used in the literature and provides a fair evaluation of the performance of machine learning models on the CIFAR-10 datasets.

It's also worth noting that cross validation can be also used for the CIFAR-10 datasets. However, the number of images in the datasets is large and most of the research paper use the original train/test split. Therefore, it's reasonable for this project to use the original train/test split.

Noted that I also use torchvision to download and process the datasets.

\subsection{Algorithms}

For the CIFAR-10 datasets, I use two classifier, ResNet-18 and Multilayer Perceptron (MLP).

\subsubsection{ResNet-18}

ResNet-18 is a popular deep convolutional neural network architecture used for image classification tasks. It was introduced by Microsoft Research in 2015 and is part of a family of ResNet models, which are designed to tackle the problem of vanishing gradients in very deep neural networks. ResNet has different variance, from ResNet-18 all the way to ResNet-152. The number after ResNet indicates the number of layers. Therefore, ResNet-18 is a relatively shallow model, with 18 layers, but it still achieves state-of-the-art performance on a range of image classification benchmarks, including ImageNet, CIFAR-10, and CIFAR-100.

For the sake of convenience, I directly use the ResNet-18 models which provides in torchvision library. Torchvision library also provides the pretrained weights. However, those weights are trained on ImageNet Datasets, not CIFAR-10. Therefore, I also compare the performance with or without using pretrained weights. There are more details in appendix.

It's worth noting that because the image size of CIFAR-10 is only 32 x 32. And ResNet is original designed for ImageNet, which image size is 224 x 224. Therefore, the first convolution layer with kernel size equals to 7 is too large for CIFAR-10 datasets, this will make lots of features lost from the first convolution layer. Moreover, the max pooling layer may also make features lost. Therefore, I change the kernel size from 7 to 3 and  also remove the max pooling layer. In the analysis section, I will discuss the performance difference after changing these two layers.

\subsubsection{MLP}

A multilayer perceptron (MLP) is a type of artificial neural network (ANN) that consists of multiple layers of interconnected nodes (also called neurons) in a feedforward network architecture. It is one of the most commonly used neural network models for supervised learning tasks such as classification, regression, and pattern recognition.


An MLP typically consists of an input layer, one or more hidden layers, and an output layer. The number of neurons in the input layer is determined by the number of features in the input data, while the number of neurons in the output layer corresponds to the number of output classes or the number of output variables in a regression task. The number of neurons in the hidden layers is a hyperparameter that can be tuned during the training process.

We know that MLP has one input layer, one output layer and several hidden layers. In this project, I will only use an MLP that consists of two hidden layers with same number of neurons. The number of neurons is also a hyperparameters that will compare in the analysis section. As for the input layer, I just flatten the images into an one dimensional vector, with $3 \times 32 \times 32 = 3072$ elements as input. For the output layer, MLP will output a one dimensional vector with 10 elements, corresponding to 10 classes.

\subsubsection{Epochs, Loss Function, Optimizer and Learning Rate Scheduler}

In the following experiments, I will use the below configuration for both MLP and ResNet-18. 

\begin{itemize}
\item Epochs: 200
\item Learning Rate: 0.1
\item Loss Function: \texttt{CrossEntropyLoss}
\item Optimizer: \texttt{SGD}
\item Learning Rate Scheduler: \texttt{ReduceLROnPlateau} (monitor on \texttt{val\_acc, patient=10, factor=0.1})
\item Batch Size: 128
\end{itemize}

Learning rate scheduler is a tool to change learning rate while training. And the LR scheduler I use will monitor on \texttt{val\_acc}. When \texttt{val\_acc} doesn't increase for 10 epochs, the learning rate will be reduced by 90\%.

\subsection{Analysis}

First of all, because CIFAR-10 already has provided a train/test split, I will not use cross-validation for this datasets.

\subsubsection{Compare Different Architecture of ResNet-18}

As I mentioned earlier, because the kernel size of first convolution layer is too large for images in CIFAR-10 datasets, I change the kernel size and also remove the first max pooling layer. This version is called ``Modified ResNet-18'', and the unmodified one is called ``Original ResNet-18''. As we can see in the chart from \autoref{chart: res_1} to \autoref{chart: res_4}, the modified one is significant better than the unmodified one. The highest test accuracy for the modified one is 0.9227, in the contrast, the unmodified one is 0.8766.

In conclusion, changing the architecture of ResNet can indeed significantly improve the model's accuracy.

\subsubsection{Comparing whether there is a pre-trained weight for ResNet-18}\label{sec: pre-trained}

PyTorch provides a pretrained weight for ResNet-18. However, the pretrained weight is trained on ImageNet datasets. Therefore, it might perform well on CIFAR-10 datasets. Because of this I make an experiment to test how the existence of pretrained weight affect the performance of model.

As we can see in \autoref{chart:resnet-18-cifar-10-pretrained-acc} and \autoref{chart:resnet-18-cifar-10-pretrained-loss}, the ResNet-18 with pretrained weight doesn't perform very well. The highest test accuracy of ResNet-18 without pretrained weight is 0.9227, as for the model with pretrained weight, this number is 0.9177. I will discuss more in \ref{sec: discuss-1}.

\subsubsection{Compare Different Architecture of MLP}

For the MLP, I fixed the number of hidden layer to two. And by setting the number of neurons in hidden layers to 100 and 2500, I trained two MLP model.

As we can see in the chart (\autoref{chart: mlp_1} to \autoref{chart: mlp_4}), the MLP with 2500 hidden neurons is significant better than the 100 ones. The highest testing accuracy for the 2500 one is 0.5886, as for the 100 ones is 0.5324.

\subsubsection{Compare Different Amount of Training Data}

In this section, I will first analysis ResNet-18 case, and then analysis MLP case. 

Noted that though the number of training data is different, the testing data for both model is exactly the same.

First, for ResNet-18, we can look at \autoref{fig:resnet-half-cifar-acc} and \autoref{fig:resnet-half-cifar-loss}. As we can observe, the ``half'' one train much more slower than the ResNet-18 that trained with full training CIFAR-10 datasets, which contains 50000 images. And the final test accuracy, the ``half'' one's accuracy is 0.8782. Which is less than the full-trained model by around 5\%.

Second, let's look at the MLP. Also we can observe that in \autoref{fig:mlp-half-cifar-acc} and \autoref{fig:mlp-half-cifar-loss}, the training accuracy of the MLP with full training datasets is slightly better, and for the testing accuracy, it is nearly the same. The ``Half MLP'' is 0.5816, and the ``Full MLP`` is 0.5886.

\subsubsection{Compare with SOTA Model}

After researching, the state-of-the-art model on CIFAR-10 is Vision Transformer (ViT). In particularly, the ViT-H/14 model can achieve 99.5\% test accuracy, which is astonishing results. However, this model require 632M parameters, in the contrast, ResNet-18 only requires 11M parameter.

In conclusion, ViT is 57 times larger than ResNet-18, but it only increase 7\% of accuracy.

\subsubsection{Overall Comparison}

From the above comparing, we know that the best performance for ResNet-18 is the modified ResNet-18 without pretrained weights. As for MLP, the best model is with 2500 hidden neurons. And the SOTA model for CIFAR-10 is ViT-H/18. 

For the overall comparison table, please refer to \autoref{tab:cifar-comp}.

\subsection{Discussion}

\subsubsection{Assessing Expected Results and Behaviors in Experiments}\label{sec: discuss-1}

\begin{enumerate}
\item \textbf{Using pretrained weights or not}

In \ref{sec: pre-trained}, I compare the performance of the ResNet-18 with or without pretrained weights. And the result is the model without pretrained weights has better performance.

One can guess the model with pretrained weights might be better, but it's not. I think one main reason is that I already changed the model architecture, therefore the idea of ``transfer learning'' didn't work well here. And also, the distribution of CIFAR-10 and ImageNet is also difference. Therefore, trained the model from scratch might be a better idea.

In conclusion, I think these are the two main reasons why the model without pretrained weights is better.

\end{enumerate}

\subsubsection{Factors Affecting Performance}

\begin{enumerate}
\item Model architecture
\item Different hyperparameters
\item Number of epochs count
\item Number of training data
\item LR scheduler

Using LR scheduler is actually really important for machine learning, especially when SGD is used. Because larger LR in the later stage of training might make the model can't converge to optimal point. Therefore it's necessary to have LR scheduler.
\end{enumerate}

\subsubsection{Future Experiments}

\begin{enumerate}
\item \textbf{Using adam optimizer instead of SGD.}

The adam optimizer can change the learning rate of each weights, which is might be a better method compared with LR decay. The SGD along with LR decay method may take longer time to converge.
\item \textbf{Apply features extractions before sending into MLP.}

Apply features extractions on images can extract useful features, and using these extracted features might be able to improve the MLP performance.
\end{enumerate}

\subsubsection{Findings and Open Questions from Experiments}

\begin{enumerate}
\item \textbf{MLP doesn't perform very well}

Before conducting the experiment, I thought it may achieve an accuracy around 70\%. But it turns out only get 60\%.

\item \textbf{Changing the architecture of the CNN is useful}

I improved the model's accuracy by 5\% by only changing the kernel size of one layer and removing one max pooling layer. This is a new inspiration for me that sometimes we need to compare the difference of the images we need to classify now with the designated image, and based on the input images to design the proper architecture. 
\end{enumerate}

\section{Public Non-Image Datasets: 20 Newgroups}
\subsection{Datasets Description}

The 20 Newsgroups datasets is a popular datasets used in natural language processing and machine learning research. It consists of a collection of approximately 20,000 documents, partitioned into 20 different newsgroups, each representing a different topic. The datasets was first collected by Ken Lang and others at the University of California, Irvine, and has since been widely used in research and experimentation.

Each document in the 20 Newsgroups datasets is a posting to one of the 20 newsgroups, and is represented as a single text file. The datasets includes a variety of topics, including politics, religion, sports, science, and technology, among others.

In this project, I will use vectorize version of this datasets. This is done by using the \texttt{CountVectorize} in scikit-learn library. In this way, the text data is transformed to real-valued vector, which can be processed by \texttt{RandomForestClassifier}, \texttt{GradientBoostingClassifier}, \texttt{AdaBoostClassifier}, etc.

As for the cross-validation, because scikit-learn has alreaday provided a train/test split, which contains 11314 train data and 7532 test data. 

\subsection{Algorithms}

In this datasets, I use four kinds of ensemble learning algorithms, Random Forest Classifier, Gradient Boosting Classifier, and AdaBoost Classifier. One SVM algorithm, linear SVM. And a MLP Classifier, which is same MLP architecture in the first datasets. Therefore, I will only talk about the four ensemble learning algorithms and the SVM algorithm.

Random Forest Classifier, Gradient Boosting Classifier, and AdaBoost Classifier are all powerful machine learning algorithms commonly used for classification tasks. These algorithms are known for their ability to handle complex datasets and provide accurate predictions.

And SVM (Support Vector Machines) is also a machine learning algorithm used for classification and regression analysis. It was developed by Vapnik and his colleagues in the 1990s. SVM is a supervised learning algorithm that works by finding the optimal hyperplane that separates the data into different classes. The hyperplane is chosen in a way that maximizes the margin between the closest data points from each class. These closest data points are known as support vectors.

These five classifiers can be found in the \texttt{scikit-learn} library. In the following experiments, I directly apply the default value of \texttt{scikit-learn} provides. I state the option clearly in different classifier.

\subsubsection{Random Forest Classifier}

Random Forest Classifier is an ensemble learning algorithm that creates multiple decision trees and combines their outputs to make a final prediction. Each tree is trained on a random subset of the features and samples of the datasets, making the algorithm robust to overfitting. The final prediction is made by taking the mode of the predictions of all trees in the forest.

\begin{itemize}
\item The number of tree: 1000 or 500　(compare the difference)
\item Criterion: Gini Impurity
\end{itemize}

\subsubsection{Gradient Boosting Classifier}

Gradient Boosting Classifier is another ensemble learning algorithm that creates multiple weak learners and combines their outputs to make a final prediction. Unlike Random Forest Classifier, it creates trees sequentially, with each new tree correcting the errors of the previous tree. This iterative process continues until a stopping criterion is met, resulting in a strong learner that makes accurate predictions.

Gradient Boosting algorithm is widely used in many machine learning competitions in Kaggle. This is reason why I choose this algorithms in this project.

\begin{itemize}
\item The Number of Tree: 100 or 50　(compare the difference)
\item Loss Function: \texttt{log\_loss}
\item Criterion: \texttt{friedman\_mse}
\item Learning Rate: 0.1
\item Max Depth: 3
\end{itemize}

\subsubsection{AdaBoost Classifier}

AdaBoost Classifier, short for Adaptive Boosting Classifier, is also an ensemble learning algorithm that creates multiple weak learners and combines their outputs to make a final prediction. It is similar to Gradient Boosting Classifier in that it creates trees sequentially, but it assigns weights to each sample in the datasets based on how difficult it is to classify correctly. Samples that are misclassified by a weak learner are given higher weights, making them more likely to be correctly classified by the next weak learner.

\begin{itemize}
\item Estimator: Decision tree with maximum depth 1
\item Number of Estimators: 50 or 500 (compare the difference)
\item Algorithm: SAMME.R
\end{itemize}

\subsubsection{Linear SVM Classifier}\label{sec: linear-svm}

Linear SVM is a specific type of SVM that assumes the data is linearly separable, meaning that a straight line can be drawn to separate the data points into different classes. Linear SVM is used when the data is linearly separable, which means that the data can be classified into two or more classes by a straight line. In this case, the SVM algorithm finds the optimal hyperplane in a way that maximizes the margin between the closest data points from each class while keeping the classification error rate as low as possible.

Linear SVM is a simple and effective algorithm that works well on a wide range of problems. However, it may not work well when the data is not linearly separable. In this case, other types of SVMs or non-linear methods may be more appropriate.

\begin{itemize}
\item Penalty: L2
\item Loss: squared hinge
\item dual: True or False (compare the difference)
\end{itemize}

\subsubsection{MLP Classifier}

Noted that this MLP Classifier is the built-in version in scikit-learn.

\begin{itemize}
\item Epochs: 200
\item Optimizer: Adam
\item Learning Rate: 0.001
\item Momentum: 0.9
\item Loss Function: \texttt{CrossEntropyLoss}
\item Tolerance: 0.0001
\item Patient: 10
\item Number of hidden layer: 1
\item Number of hidden neurons: 100
\end{itemize}

\subsection{Analysis}

\subsubsection{Compare Different Random Forest Classifier}

As we can see in the \autoref{chart:random-forest-500-acc} to \autoref{chart:random-forest-1000-conf}, obviously, the one with more estimators has higher accuracy. However, the difference between the accuracy is not much, only 0.46\%. The accuracy random forest classifier with 1000 estimators is 74.65\%.

\subsubsection{Compare Different Gradient Boosting Classifier}

As we can see in the \autoref{chart:gb-100-acc} to \autoref{chart:gb-50-conf}, the accuracy of gradient boosting classifier with 100 estimators is 70.74\%, which is higher than one that with 50 estimators.
\subsubsection{Compare Different AdaBoost Classifier}

As we can see in the \autoref{chart:ada-50-acc} to \autoref{chart:ada-500-conf}, the accuracy of Adaboost classifier with 50 estimators is 46.52\%, which is higher than one that with 500 estimators. This is not the results I expected, I will discuss about it in the discussion section.

\subsubsection{Compare Different Linear SVM Classifier}

As we can see in the \autoref{chart:svm-primal-acc} to \autoref{chart:svm-primal-conf}, using dual or primal optimization is actually the same for 20 newsgroups datasets. And also the accuracy of SVM is 78.47\% and AUC is 47.85\%, which is a high accuracy and AUC.

\subsubsection{Compare Different MLP Classifier}

As we can see in \autoref{chart:mlp-100-acc} to \autoref{chart:mlp-50-conf}, the MLP with 50 hidden neurons perform better than one that with 100 hidden neurons, with accuracy is 78.57\%. 

\subsubsection{Compare Different Amount of Training Data}

In this section, I will only test on the best model, which is the MLP model with 50 hidden neurons. Recall that the accuracy of linear SVM model with full train datasets is 78.57\%.

As we can see in the \autoref{fig:mlp-half-acc} and \autoref{fig:mlp-half-conf}, the testing accuracy is 0.7767 which is a little bit less than the full trained one.

\subsubsection{Compare with SOTA Model}

Currently, to my best knowledge, the SOTA of 20 newsgroups datasets is \href{https://arxiv.org/abs/2211.02563v1}{LinearSVM+TFIDF}. TFIDF is a kind of vectorizer and LinearSVM is the same algorithms as in \ref{sec: linear-svm}.

\subsubsection{Overall Comparison}

You can refer to \autoref{tab:20-news}.

By this table, we know that best model is MLP model with 78.57\% test accuracy.

\subsection{Discussion}
\subsubsection{Assessing Expected Results and Behaviors in Experiments}

Before conducting the experiment, I guess the gradient boosting or Adaboost algorithm may perform pretty well. Because I heard that GB algorithms perform really well on lots of Kaggle competition. However, after the experiments, I found that the best model is simple linear SVM classifier, and it only took around 2 minutes to finished training, which is really surprising.

Second, I also expect adaboost with more estimators\footnote{Estimator is actually a decision tree, which is a weak classifier. Therefore more classifier may make adaboost algorithm to overfit the training data.} would perform better. After some researching, I found that more estimators may lead to overfitting problem, which made the testing accuracy worse than the less one.

Third, the MLP case is also surprised me, the 50 one perform better than the 100 one. I guess this is also because of overfitting problem. 

To overcome overfitting problem, one usual strategy is using ``early-stopping''. Stopping the training process before the model becoming to be overfitted.

\subsubsection{Factors Affecting Performance}

\begin{enumerate}
\item Different algorithms
\item The latent distribution of data
\item Different vectorizer and preprocessing method
\end{enumerate}

\subsubsection{Future Experiments}

\begin{enumerate}
\item \textbf{Try to use different vectorizer}
\item \textbf{Try more text preprocessing}

Including removing stop words, stemming, etc.

\item \textbf{Try to use NLP Model like BERT}
\item \textbf{Using early-stopping to avoid overfitting}
\end{enumerate}

\subsubsection{Findings and Open Questions from Experiments}

\begin{enumerate}
\item Linear SVM classifier is a great model for some cases
\item Histogram-based gradient boosting is time consuming
\item For simple classification task, the traditional machine learning can already perform really well
\end{enumerate}

\section{Self-made Datasets: Satellite Images of 5 Regions}
\subsection{Datasets Description}

In this datasets, I collect the satellite images of mountain area from 5 regions over the world, including Taiwan, Canada, Himalaya, Hengduan and Argentina. The task is to classify the satellite images to these five regions. The satellite images are from MapTiler, a tile map service. I first calculate which tiles should be download, then combine those tiles into a PNG file. Each image is an $256 \times 256$ RGB images.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figure/terrain_regions}
\caption{This figure shows 5 satellite images from 5 regions}
\label{fig:terrainregions}
\end{figure}

\subsection{Algorithms}

Because this datasets is also a image datasets, I just use the two classifiers (ResNet-18 and MLP) which used in the CIFAR-10 datasets with the exact same hyperparameters and configurations.

For MLP, because directly use $3 \times 256 \times 256$ as input layer is too large, I down-sampling the image from 256 x 256 to 128 x 128.

\subsection{Analysis}

Because I've already conducted the experiments of comparing different architecture of the same classifier, I will only compare the difference between ResNet-18 and MLP. And also compare different amount of training data.

For the validation, because this datasets is self-made datasets, I use 5-fold cross-validation to evaluate the model performance.

\subsubsection{Compare ResNet-18 and MLP}

As we can see in \autoref{tab:terrain-comp}, MLP outperform ResNet-19. The mean accuracy of MLP is 98.032\%. As for ResNet-18, it's 93.186\%. 

\subsubsection{Compare with SOTA Model}

Because this datasets is a self-made datasets, I can't compare to other models. However, there is some similar tasks, using CNN and ViT.

\subsection{Discussion}
\subsubsection{Assessing Expected Results and Behaviors in Experiments}

I am actually surprised by the performance of ResNet-18, I thought it can only achieve an accuracy around 90\%. However, most of the fold can achieve 100\% of testing accuracy.

Furthermore, simple MLP can even outperform ResNet-18, which is also a big surprise for me.

\subsubsection{Factors Affecting Performance}

\begin{enumerate}
\item Size of datasets
\item Different model
\end{enumerate}

\subsubsection{Future Experiments}

\begin{enumerate}
\item \textbf{More robust training and testing data}

This classify problem is too simple for ResNet-18, an unmodified model can achieve near 100\% of accuracy. Therefore, I would like to introduce more data or regions to made this classify problem more challenging.

\item \textbf{Optimized the model architecture}
\item \textbf{Try to use ViT}

\end{enumerate}

\subsubsection{Findings and Open Questions from Experiments}

\begin{enumerate}
\item Small model may perform well
\item Why this classification problem is that easy
\end{enumerate}

\clearpage
\pagenumbering{arabic}% resets `page` counter to 1
\renewcommand*{\thepage}{A. \arabic{page}}
\begin{appendices}
\section{Figures, Charts and Tables}

\subsection{Public Image Datasets: CIFAR-10}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/Section-2-Panel-0-uzzeut0la}
\caption{Training accuracy of two ResNet-18 models on CIFAR-10 datasets}
\label{chart: res_1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/Section-2-Panel-1-crmf3l46q}
\caption{Training loss of two ResNet-18 models on CIFAR-10 datasets}
\label{chart: res_2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/Section-2-Panel-2-qves7h50b}
\caption{Testing accuracy of two ResNet-18 models on CIFAR-10 datasets}
\label{chart: res_3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/Section-2-Panel-3-cw0a3tnrd}
\caption{Testing loss of two ResNet-18 models on CIFAR-10 datasets}
\label{chart: res_4}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/resnet-18-cifar-10-pretrained-acc}
\caption{Training and testing accuracy of two ResNet-18 model with and without ImageNet pretrained weight}
\label{chart:resnet-18-cifar-10-pretrained-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/resnet-18-cifar-10-pretrained-loss}
\caption{Training and testing loss of two model with and without ImageNet pretrained weight}
\label{chart:resnet-18-cifar-10-pretrained-loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/resnet-cifar-auc}
\caption{Accuracy table of pretrained ResNet-18}
\label{chart:resnet-cifar-auc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/resnet-cifar-conf"}
\caption{Confusion matrix of pretrained ResNet-18}
\label{fig:resnet-cifar-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp_cifar_10_train_acc}
\caption{Training accuracy of two MLP models on CIFAR-10 datasets}
\label{chart: mlp_1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp_cifar_10_train_loss}
\caption{Training loss of two MLP models on CIFAR-10 datasets}
\label{chart: mlp_2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp_cifar_10_test_acc}
\caption{Testing accuracy of two MLP models on CIFAR-10 datasets}
\label{chart: mlp_3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp_cifar_10_test_loss}
\caption{Testing loss of two MLP models on CIFAR-10 datasets}
\label{chart: mlp_4}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/resnet-half-cifar-acc}
\caption{Training and testing accuracy of ResNet-18 with different training data amount. The ``Half-ResNet-18'' is trained with only 25000 training images.}
\label{fig:resnet-half-cifar-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/resnet-half-cifar-loss}
\caption{Training and testing loss of ResNet-18 with different training data amount. The ``Half-ResNet-18'' is trained with only 25000 training images.}
\label{fig:resnet-half-cifar-loss}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp-half-cifar-acc}
\caption{Training and testing accuracy of MLP with different training data amount. The ``Half MLP'' is trained with only 25000 training images.}
\label{fig:mlp-half-cifar-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp-half-cifar-loss}
\caption{Training and testing accuracy of MLP with different training data amount. The ``Half MLP'' is trained with only 25000 training images.}
\label{fig:mlp-half-cifar-loss}
\end{figure}

\begin{table*}[h]
\centering
\caption{Overall Comparision of Models Trained on CIFAR-10 Datasets}
\label{tab:cifar-comp}
\begin{tabular}{@{}ccc@{}}
Model                      & Feature                                       & Test Accuracy   \\ \midrule
\multirow{3}{*}{ResNet-18} & w/ pretrained weights                         & 0.9177          \\
                           & w/o pretrained weights                        & \textbf{0.9227} \\
                           & w/o pretrained weights and half training size & 0.8782          \\ \midrule
\multirow{3}{*}{MLP}       & 2500 hidden neurons                           & \textbf{0.5886} \\
                           & 100 hidden neurons                            & 0.5324          \\
                           & 2500 hidden neurons and half training size    & 0.5816          \\ \midrule
ViT-H/14                   &                                               & \textbf{0.995} 
\end{tabular}
\end{table*}

\subsection{Public Non-image Datasets: 20 Newsgroups}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/random-forest-500-acc}
\caption{The test accuracy of each classes of the random forest classifier with 500 tress.}
\label{chart:random-forest-500-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Random Forest 500"}
\caption{The confusion matrix of each class of the random forest classifier with 500 tress.}
\label{chart:random-forest-500-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/random-forest-1000-acc}
\caption{The test accuracy of each class of the random forest classifier with 1000 trees.}
\label{chart:random-forest-1000-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Random Forest 1000"}
\caption{The confusion matrix of each class of the random forest classifier with 1000 tress.}
\label{chart:random-forest-1000-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/gb-100-acc}
\caption{The test accuracy of each class of the gradient boosting classifier with 100 estimators.}
\label{chart:gb-100-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Gradient Boosting 100 estimators"}
\caption{The confusion matrix of each classof the gradient boosting classifier with 100 estimators.}
\label{chart:gb-100-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/gb-50-acc}
\caption{The test accuracy of each class of the gradient boosting classifier with 50 estimators.}
\label{chart:gb-50-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Gradient Boosting 50 estimators"}
\caption{The confusion matrix of each class of the gradient boosting classifier with 50 estimators.}
\label{chart:gb-50-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/ada-500-acc}
\caption{The test accuracy of each class of the Adaboost classifier with 500 estimators.}
\label{chart:ada-500-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Adaboost with 500 Estimators"}
\caption{The confusion matrix of each class of the Adaboost classifier with 500 estimators.}
\label{chart:ada-500-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/ada-50-acc}
\caption{The test accuracy of each class of Adaboost with 50 estimators.}
\label{chart:ada-50-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Adaboost with 50 Estimators"}
\caption{The confusion matrix of each class of the gradient boosting classifier with 50 estimators.}
\label{chart:ada-50-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/svm-primal-acc}
\caption{The test accuracy of each class of the linear SVM classifier with primal optimization.}
\label{chart:svm-primal-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Linear SVC with Primal Optimization"}
\caption{The confusion matrix of each class of the linear SVM classifier with primal optimization.}
\label{chart:svm-primal-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/svm-dual-acc}
\caption{The test accuracy of each class of the linear SVM classifier with dual optimization.}
\label{chart:svm-dual-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/Linear SVC with Dual Optimization"}
\caption{The confusion matrix of each class of the linear SVM classifier with dual optimization.}
\label{chart:svm-dual-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp-100-acc}
\caption{The test accuracy of each class of the MLP classifier with 100 hidden neurons.}
\label{chart:mlp-100-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/MLP 100"}
\caption{The confusion matrix of each class of the MLP classifier with 100 hidden neurons.}
\label{chart:mlp-100-conf}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp-50-acc}
\caption{The test accuracy of each class of the MLP classifier with 50 hidden neurons.}
\label{fig:mlp-50-acc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{"charts/MLP 50"}
\caption{The confusion matrix of each class of the MLP classifier with 50 hidden neurons.}
\label{chart:mlp-50-conf}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{charts/mlp-half-acc}
\caption{The test accuracy of each class of the MLP classifier with 50 hidden neurons and half training data size.}
\label{fig:mlp-half-acc}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{"charts/MLP 50 Half"}
\caption{The confusion matrix of each class of the MLP classifier with 50 hidden neurons and half training data size.}
\label{fig:mlp-half-conf}
\end{figure}


\begin{table*}[h]
\centering
\caption{Overall Comparision of Models Trained on 20 Newsgroups Datasets}
\label{tab:20-news}
\begin{tabular}{@{}ccccccc@{}}
Model                              & Feature                          & Test Accuracy   & AUC             & Precision     & Recall        & F1-Score      \\ \midrule
\multirow{2}{*}{Random Forest}     & 500 esimators                    & 0.7419          & \textbf{0.5075} & 0.73          & 0.75          & 0.72          \\
                                   & 1000 estimators                  & 0.7465          & 0.5039          & 0.73          & 0.76          & 0.73          \\ \midrule
\multirow{2}{*}{Gradient Boosting} & 50 estimators                    & 0.6929          & 0.5058          & 0.68          & 0.73          & 0.70          \\
                                   & 100 estimators                   & 0.7074          & 0.5034          & 0.70          & 0.74          & 0.71          \\ \midrule
\multirow{2}{*}{Adaboost}          & 50 esimators                     & 0.4652          & 0.5060          & 0.46          & 0.65          & 0.50          \\
                                   & 500 esimators                    & 0.3952          & 0.4893          & 0.39          & 0.44          & 0.41          \\ \midrule
\multirow{2}{*}{Linear SVM}        & Primal Optimization              & 0.7847          & 0.4785          & \textbf{0.78} & \textbf{0.78} & 0.77          \\
                                   & Dual Optimization                & 0.7847          & 0.4785          & \textbf{0.78} & \textbf{0.78} & 0.77          \\ \midrule
\multirow{3}{*}{MLP}               & 50 neurons                       & \textbf{0.7857} & 0.4832          & \textbf{0.78} & \textbf{0.78} & \textbf{0.78} \\
                                   & 50 neurons w/ half training data & 0.7767          & 0.4883          & 0.77          & 0.77          & 0.77          \\
                                   & 100 neurons                      & 0.7820          & 0.4820          & 0.77          & 0.78          & 0.77         
\end{tabular}
\end{table*}





\subsection{Self-made Datasets: Satellite Images of 5 Regions}

\begin{table}[H]
\centering
\caption{The 5-fold test accuracy for ResNet-18 and MLP. Can see that MLP has better test accuracy.}
\label{tab:terrain-comp}
\begin{tabular}{@{}cccc@{}}
\toprule
Model                      & Fold & Test Accuracy & Mean Accuracy            \\ \midrule
\multirow{5}{*}{MLP}       & 1    & 1.0           & \multirow{5}{*}{\textbf{0.98032}} \\
                           & 2    & 1.0           &                          \\
                           & 3    & 0.9094        &                          \\
                           & 4    & 0.9969        &                          \\
                           & 5    & 0.9953        &                          \\ \midrule
\multirow{5}{*}{ResNet-18} & 1    & 0.8375        & \multirow{5}{*}{0.93186} \\
                           & 2    & 0.9703        &                          \\
                           & 3    & 0.9703        &                          \\
                           & 4    & 1.0           &                          \\
                           & 5    & 0.8812        &                          \\ \bottomrule
\end{tabular}
\end{table}

\section{Some Note about the Python Scripts}

Because I'm using jupyter notebook for training and testing, the code below is actually the content of a jupyter notebook. Therefore, it can't be directly execute as an python code. Each \texttt{\# In[...]: } is the beginning of a cell. Each cell should be execute independently and the execution order of cell is not determine by the order.

\section{Code of CIFAR-10}

\begin{code}
\captionof{listing}{Training and Testing of CIFAR-10}
\label{code:cifar-10}
\begin{minted}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.backends.cudnn as cudnn

import torchvision as tv
import torchvision.transforms as transforms

import wandb

import os
import numpy as np

from tqdm import *


# In[2]:


wandb.login()


# In[3]:


lr = 0.1
best_acc = 0.0
start_epoch = 0


# In[ ]:





# In[4]:


device = 'cuda' if torch.cuda.is_available() else 'cpu'


# In[5]:


# Transforms 

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])


# In[6]:


# Datasets
train_dataset = tv.datasets.CIFAR10(root='./datasets', train=True, transform=transform_train, download=True)
train_dataset_half = torch.utils.data.random_split(train_dataset, [25000, 25000])[0]
test_dataset = tv.datasets.CIFAR10(root='./datasets', train=False, transform=transform_test, download=True)

# Data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=35)
train_loader_half = torch.utils.data.DataLoader(dataset=train_dataset_half, batch_size=128, shuffle=True, num_workers=35)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=35)


# In[8]:


print(len(train_dataset))
print(len(test_dataset))
print(len(train_dataset_half))


# In[9]:


# Define Model
net = tv.models.resnet18(pretrained=True)
net.fc = nn.Linear(in_features=512, out_features=10, bias=True)

net.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)
net.maxpool = nn.Identity()


# In[26]:


class MLP(nn.Module):
    def __init__(self, n_hidden_nodes):
        super(MLP, self).__init__()
        self.n_hidden_nodes = n_hidden_nodes
        # Set up perceptron layers and add dropout
        self.fc1 = nn.Linear(3 * 32 * 32,
                                   n_hidden_nodes)
        self.fc1_drop = nn.Dropout(0.2)
        self.fc2 = nn.Linear(n_hidden_nodes, n_hidden_nodes)
        self.fc2_drop = nn.Dropout(0.2)
        self.out = nn.Linear(n_hidden_nodes, 10)

    def forward(self, x):
        x = x.view(-1, 3 * 32 * 32)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc1_drop(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc2_drop(x)
        return self.out(x)
    
net = MLP(100)


# In[10]:


net = net.to(device)


# In[11]:


lr = 0.1
best_acc = 0.0
start_epoch = 0

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=lr,
                      momentum=0.9, weight_decay=5e-4, nesterov=True)
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, 
                                                       threshold=0.001, mode="max") 

wandb.init(
    project="AI Capstone Project 1",
    config = {
        "lr": 0.1,
        "arch": "resnet18",
        "dataset": "CIFAR-10",
        "epochs": 200,
        "patient": 10,
        "pretrained": True,
        "training data": 25000
    }
)

# Training
def train(epoch):
    print('\nEpoch: %d' % epoch)
    net.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader_half)):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "train_acc": 1.*correct/total,
        "lr": scheduler.optimizer.param_groups[0]['lr'],
    })
        
    


def test(epoch):
    global best_acc
    net.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in tqdm(enumerate(test_loader)):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
#                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))

    # Save checkpoint.
    acc = 1.*correct/total
    wandb.log({
        "epoch": epoch,
        "test_loss": test_loss,
        "test_acc": acc,
    })
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net.state_dict(),
            'acc': acc,
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state, './checkpoint/ckpt.pth')
        wandb.save('./checkpoint/ckpt.pth')
        best_acc = acc
    scheduler.step(acc)


for epoch in range(start_epoch, start_epoch+200):
    train(epoch)
    test(epoch)
    print(best_acc)
    


# In[30]:


wandb.finish()


# In[16]:


best_model = wandb.restore('checkpoint/ckpt.pth', run_path="jayinnn/AI Capstone Project 1/1zzr5u3j")
print(best_model)


# In[26]:


net = net.to("cpu")
net.load_state_dict(torch.load("checkpoint/half-resnet.pth")["net"])
net.eval()


# In[97]:


from sklearn import metrics
from sklearn.metrics import *
import matplotlib.pyplot as plt
import itertools

def plot_training_metrics(model, y_actual, y_pred, classes, model_name='model'):
    """
    Input: trained model history, model, test image generator, actual and predicted labels, class list
    Output: Plots loss vs epochs, accuracy vs epochs, confusion matrix
    """
    fpr, tpr, thresholds = metrics.roc_curve(y_actual, y_pred, pos_label=9)
    AUC       = metrics.auc(fpr, tpr)*100
    Acc       = metrics.accuracy_score(y_actual, y_pred)*100 
    results_title =(f"\n Model AUC {AUC:.2f}%, Accuracy {Acc:.2f}% on Test Data\n")
    print(results_title.format(AUC, Acc))

    
    # print classification report
    print(classification_report(y_actual, y_pred, target_names=classes))


    plt.subplots(figsize=(12,12))
    


    
     # calculate Confusion Matrix
    cm = confusion_matrix(y_actual, y_pred)

    # create confusion matrix plot
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.BuPu)
    plt.title(f"{model_name} Confusion Matrix \nAUC: {AUC:.2f}%")
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    # loop through matrix, plot each 
    threshold = cm.max() / 2.
    for r, c in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(c, r, format(cm[r, c], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[r, c] > threshold else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.savefig(f"{model_name}.pdf")

    plt.show()

def accuracy(predicted, actual):
    _, predictions = torch.max(predicted, dim=1)
    return torch.tensor(torch.sum(predictions==actual).item()/len(predictions))
    
@torch.no_grad()
def get_all(model, loader):
    all_preds = torch.tensor([])
    all_truths = torch.tensor([])
    for batch in loader:
        images, labels = batch

        preds = model(images)
        all_preds = torch.cat((all_preds, preds) ,dim=0)
        all_truths = torch.cat((all_truths, labels), dim=0)

    return all_preds, all_truths


# In[91]:


net = net.to("cpu")
net.load_state_dict(torch.load("checkpoint/resnet18-cifar.pth")["net"])
net.eval()


# In[92]:


y_pred, y_truth = get_all(net, test_loader)


# In[63]:


classes = '''airplane automobile bird cat deer dog frog horse ship truck'''.split()


# In[93]:


_, y_pred_one = y_pred.max(1)


# In[77]:


print(y_pred_one)


# In[98]:


plot_training_metrics(net, y_truth, y_pred_one, classes, model_name="Pretrained ResNet-18")

\end{minted}
\end{code}

\section{Code of 20 Newsgroups}

\begin{code}
\captionof{listing}{Training and Testing of 20 Newsgroups}
\label{code:20-news}
\begin{minted}
#!/usr/bin/env python
# coding: utf-8

# In[107]:


from sklearn.datasets import fetch_20newsgroups, fetch_20newsgroups_vectorized
from sklearn.linear_model import Perceptron
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier


# In[18]:


train_x, train_y = fetch_20newsgroups_vectorized(
    data_home='./datasets', 
    subset="train", 
    return_X_y=True, 
    remove=("headers")
)


# In[22]:


test_x, test_y = fetch_20newsgroups_vectorized(
    data_home='./datasets', 
    subset="test", 
    return_X_y=True, 
    remove=("headers")
)


# In[19]:


train_x


# In[118]:


print(len(train_y))
print(len(test_y))


# In[84]:


perc = Perceptron(
    max_iter = 2000,
    n_jobs = 30,
    validation_fraction = 0.1,
    n_iter_no_change = 2000,
)
perc.fit(train_x, train_y)


# In[95]:


plot_training_metrics(perc.predict(test_x), test_y, classes, model_name="Perceptron")


# In[85]:


rf = RandomForestClassifier(
    n_jobs = 30,
    verbose = 1,
    n_estimators=1000,
)
rf.fit(train_x, train_y)


# In[100]:


rf500 = RandomForestClassifier(
    n_jobs = 30,
    verbose = 1,
    n_estimators=500,
)
rf500.fit(train_x, train_y)


# In[113]:


plot_training_metrics(rf.predict(test_x), test_y, classes, model_name="Random Forest 1000")


# In[106]:


plot_training_metrics(rf500.predict(test_x), test_y, classes, model_name="Random Forest 500")


# In[87]:


gb = GradientBoostingClassifier(
    verbose = 1,
    n_estimators=100,
)
gb.fit(train_x, train_y)


# In[99]:


gb50 = GradientBoostingClassifier(
    verbose = 1,
    n_estimators=50,
)
gb50.fit(train_x, train_y)


# In[116]:


plot_training_metrics(gb50.predict(test_x), test_y, classes, model_name="Gradient Boosting 50 estimators")


# In[ ]:





# In[ ]:


mlp = MLPClassifier(
    hidden_layer_sizes=(100),
    verbose=True
) 


# In[68]:


mlp.fit(train_x, train_y)


# In[134]:


mlp50 = MLPClassifier(
    hidden_layer_sizes=(50),
    verbose=True
)
mlp50.fit(train_x, train_y)


# In[141]:


train_x_half, _, train_y_half, _ = sklearn.model_selection.train_test_split(train_x, train_y, random_state=42)


# In[139]:


train_x_half


# In[ ]:


mlp50_half = mlp50 = MLPClassifier(
    hidden_layer_sizes=(50),
    verbose=True
)
mlp50_half.fit(train_x_half, train_y_half)


# In[135]:


plot_training_metrics(mlp50_half.predict(test_x), test_y, classes, model_name="MLP 50 Half")


# In[129]:


from sklearn.svm import LinearSVC

linearsvc = LinearSVC(
    verbose = 1,
    dual=True,
)

linearsvc.fit(train_x, train_y)


# In[126]:


from sklearn.cross_validation import train_test_split


# In[130]:


plot_training_metrics(linearsvc.predict(test_x), test_y, classes, model_name="Linear SVC with Dual Optimization")


# In[80]:


from sklearn.metrics import f1_score, confusion_matrix, classification_report


# In[ ]:





# In[111]:


ada = AdaBoostClassifier(
    n_estimators=500
)
ada.fit(train_x, train_y)


# In[119]:


ada50 = AdaBoostClassifier(
    n_estimators=50
)
ada50.fit(train_x, train_y)


# In[133]:


plot_training_metrics(ada50.predict(test_x), test_y, classes, model_name="Adaboost with 50 Estimators")


# In[ ]:


model = HistGradientBoostingClassifier()


# In[58]:


model.fit(train_x, train_y)


# In[59]:


model.score(test_x, test_y)


# In[91]:


classes = fetch_20newsgroups_vectorized(
    data_home='./datasets', 
    subset="test", 
    remove=("headers")
)["target_names"]


# In[92]:


classes


# In[94]:


from sklearn import metrics
from sklearn.metrics import *
import matplotlib.pyplot as plt
import itertools
import numpy as np

def plot_training_metrics(y_actual, y_pred, classes, model_name='model'):
    """
    Input: trained model history, model, test image generator, actual and predicted labels, class list
    Output: Plots loss vs epochs, accuracy vs epochs, confusion matrix
    """
    fpr, tpr, thresholds = metrics.roc_curve(y_actual, y_pred, pos_label=9)
    AUC       = metrics.auc(fpr, tpr)*100
    Acc       = metrics.accuracy_score(y_actual, y_pred)*100 
    results_title =(f"\n Model AUC {AUC:.2f}%, Accuracy {Acc:.2f}% on Test Data\n")
    print(results_title.format(AUC, Acc))

    
    # print classification report
    print(classification_report(y_actual, y_pred, target_names=classes))


    
#     # create plots
    plt.subplots(figsize=(12,12))
    

    
     # calculate Confusion Matrix
    cm = confusion_matrix(y_actual, y_pred)

    # create confusion matrix plot
#     plt.subplot(1,3,3)
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.BuPu)
    plt.title(f"{model_name} Confusion Matrix \nAUC: {AUC:.2f}%")
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    # loop through matrix, plot each 
    threshold = cm.max() / 2.
    for r, c in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(c, r, format(cm[r, c], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[r, c] > threshold else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.savefig(f"{model_name}.pdf")

    plt.show()

def accuracy(predicted, actual):
    _, predictions = torch.max(predicted, dim=1)
    return torch.tensor(torch.sum(predictions==actual).item()/len(predictions))
    
def get_all(model, loader):
    all_preds = torch.tensor([])
    all_truths = torch.tensor([])
    for batch in loader:
        images, labels = batch

        preds = model(images)
        all_preds = torch.cat((all_preds, preds) ,dim=0)
        all_truths = torch.cat((all_truths, labels), dim=0)

    return all_preds, all_truths
\end{minted}
\end{code}
\section{Code of Satellite Image Datasets}

\begin{code}
\captionof{listing}{Collecting Satellite Image Datasets}
\label{code:collect}
\begin{minted}
import math
import urllib.request
import os
import glob
import subprocess
import shutil
# import requests
from tqdm import tqdm

# from fp.fp import FreeProxy
from tile_convert import bbox_to_xyz, tile_edges
from osgeo import gdal





def download_tile(x, y, z, tile_server):
    url = tile_server.replace(
        "{x}", str(x)).replace(
        "{y}", str(y)).replace(
        "{z}", str(z))
    path = '{}/{}_{}_{}.jpg'.format(temp_dir, x, y, z)
    if(os.path.isfile(path)):
        return path
    # print("getting proxy")
    # proxies = {"http": FreeProxy(rand=True).get()}
    # proxy = urllib.request.ProxyHandler(pro)
    # opener = urllib.request.build_opener(proxy)
    # urllib.request.install_opener(opener)
    # print(proxies)
    # r = requests.get(url)
    # print(path)
    # print(r.content)
    # with open(path, 'wb') as f:
        # f.write(r.content)
    urllib.request.urlretrieve(url, path)
    return(path)


def merge_tiles(input_pattern, output_path):
    merge_command = ['gdal_merge.py', '-o', output_path]

    for name in glob.glob(input_pattern):
        merge_command.append(name)

    subprocess.call(merge_command)


def georeference_raster_tile(x, y, z, path):
    bounds = tile_edges(x, y, z)
    filename, extension = os.path.splitext(path)
    gdal.Translate(filename + '.tif',
                   path,
                   outputSRS='EPSG:4326',
                   outputBounds=bounds)

def crop(input, output, lat, lon):
    gdal.Translate(
        output,
        input,
        outputSRS='EPSG:4326',
        projWin=[lon, lat+1, lon+1, lat]
    )

taiwan = [
    (23, 120),
    (23, 121),
    (24, 121)
]

china = [
    (26, 101),
    (26, 100),
    (26, 99),
    (26, 98),
    (29, 101),
    (29, 100),
    (29, 99),
    (28, 101),
    (28, 100),
    (28, 99),
    (28, 98),
    (27, 101),
    (27, 100)
]

hima = [
    (29, 81),
    (29, 82),
    (29, 83),
    (28, 83),
    (28, 84),
    (28, 85),
    (27, 85),
    (27, 86),
    (28, 86),
    (27, 87)
]

arge = [
    (-48, -74),
    (-49, -74),
    (-49, -75),
    (-50, -74),
    (-50, -75),
    (-51, -73),
    (-51, -74),
    (-51, -75),
    (-52, -74),
]

cana = [
    (58, -134),
    (57, -133),
    (56, -132),
    (56, -131),
    (55, -131),
]

#---------- CONFIGURATION -----------#
# tile_server = "https://mt0.google.com/vt/lyrs=s&x={x}&y={y}&z={z}"
region = "cana"
tile_server = f"https://api.maptiler.com/tiles/satellite/{z}/{x}/{y}.jpg?key={key}"
temp_dir = os.path.join(os.path.dirname(__file__), 'temp/'+region)
output_dir = os.path.join(os.path.dirname(__file__), 'output/'+region)
zoom = 15

# lon_max = 122
# lat_max = 25
#-----------------------------------#
for lat_min, lon_min in cana:
    print(lat_min, lon_min)

    lat_dir = ("N" if lat_min > 0 else "S")
    lon_dir = ("E" if lon_min > 0 else "W")

    lat_max = lat_min + 1
    lon_max = lon_min + 1
    x_min, x_max, y_min, y_max = bbox_to_xyz(
        lon_min, lon_max, lat_min, lat_max, zoom)

    print("Downloading {} tiles".format((x_max - x_min + 1) * (y_max - y_min + 1)))

    pbar = tqdm(total=(x_max - x_min + 1) * (y_max - y_min + 1))
    for x in range(x_min, x_max + 1):
        for y in range(y_min, y_max + 1):
            # print("{},{}".format(x, y))
            pbar.update(1)
            pbar.set_description("{},{}".format(x, y))
            png_path = download_tile(x, y, zoom, tile_server)
            georeference_raster_tile(x, y, zoom, png_path)

    pbar.close()
    print("Download complete")

    print("Merging tiles")
    merge_tiles(temp_dir + '/*.tif', output_dir + '/{}{}{}{}_{}.tif'.format(lat_dir, abs(lat_min), lon_dir, abs(lon_min), zoom))
    print("Merge complete")

    print("Croping")
    
    crop(
        output_dir + '/{}{}{}{}_{}.tif'.format(lat_dir, abs(lat_min), lon_dir, abs(lon_min), zoom), 
        output_dir + '{}{}{}{}_{}_final_sat.tif'.format(lat_dir, abs(lat_min), lon_dir, abs(lon_min), zoom), 
        lat_min, 
        lon_min
    )
    print("Crop complete")


    shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)
\end{minted}
\end{code}

\begin{code}
\captionof{listing}{Training and Testing of Terrain Datasets}
\label{code:terrain}
\begin{minted}
#!/usr/bin/env python
# coding: utf-8

# In[5]:


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.backends.cudnn as cudnn

import torchvision as tv
import torchvision.transforms as transforms

from sklearn.model_selection import KFold

import wandb

import os
import numpy as np

from tqdm import *


# In[6]:


import wandb
wandb.login()


# In[7]:


lr = 0.1
best_acc = 0.0
start_epoch = 0


# In[8]:


device = 'cuda:1' if torch.cuda.is_available() else 'cpu'


# In[53]:


# Transforms 

transform_train = transforms.Compose([
    transforms.Resize((128, 128)),
    # transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),
])

transform_test = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),
])


# In[54]:


train_dataset = tv.datasets.ImageFolder(root='./datasets/terrain_train', transform=transform_train)
test_dataset = tv.datasets.ImageFolder(root='./datasets/terrain_test', transform=transform_test)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, num_workers=35)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False, num_workers=35)


# In[51]:


datasets = torch.utils.data.ConcatDataset([train_dataset, test_dataset])
datasets_half = torch.utils.data.random_split(datasets, [1600, 1600])[0]


# In[34]:


print(len(datasets))


# In[22]:


print(len(train_dataset))
print(len(test_dataset))


# In[30]:


# Define Model
# net = timm.create_model("hf_hub:edadaltocg/resnet18_cifar10")
net = tv.models.resnet18(pretrained=False)
net.fc = nn.Linear(in_features=512, out_features=5, bias=True)

# net.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)
# net.maxpool = nn.Identity()
net = net.to(device)


# In[55]:


class MLP(nn.Module):
    def __init__(self, n_hidden_nodes):
        super(MLP, self).__init__()
        self.n_hidden_nodes = n_hidden_nodes
        # Set up perceptron layers and add dropout
        self.fc1 = nn.Linear(3 * 128 * 128,
                                   n_hidden_nodes)
        self.fc1_drop = nn.Dropout(0.2)
        self.fc2 = nn.Linear(n_hidden_nodes, n_hidden_nodes)
        self.fc2_drop = nn.Dropout(0.2)
        self.out = nn.Linear(n_hidden_nodes, 5)

    def forward(self, x):
        x = x.view(-1, 3 * 128 * 128)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc1_drop(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc2_drop(x)
        return self.out(x)
    
net = MLP(200)


# In[56]:


net = net.to(device)


# In[24]:


lr = 0.1
best_acc = 0.0
start_epoch = 0

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=lr,
                      momentum=0.9, weight_decay=5e-4, nesterov=True)
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, 
                                                       threshold=0.001, mode="max") 

wandb.init(
    project="AI Capstone Project 1",
    config = {
        "lr": 0.1,
        "arch": "mlp",
        "dataset": "terrain",
        "hidden_nodes": 1000,
        "epochs": 200,
        "patient": 10,
    }
)


# Training
def train(epoch):
    print('\nEpoch: %d' % epoch)
    net.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader)):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "train_acc": 1.*correct/total,
        "lr": scheduler.optimizer.param_groups[0]['lr'],
    })
        
    
def test(epoch):
    global best_acc
    net.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in tqdm(enumerate(test_loader)):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
#                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))

    # Save checkpoint.
    acc = 1.*correct/total
    wandb.log({
        "epoch": epoch,
        "test_loss": test_loss,
        "test_acc": acc,
    })
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net.state_dict(),
            'acc': acc,
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state, './checkpoint/ckpt.pth')
        wandb.save('./checkpoint/ckpt.pth')
        best_acc = acc
    scheduler.step(acc)


for epoch in range(start_epoch, start_epoch+200):
    train(epoch)
    test(epoch)
    print(best_acc)
    


# In[31]:


split = KFold(n_splits=5,shuffle=True,random_state=42)


# In[45]:


criterion = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=lr,
#                       momentum=0.9, weight_decay=5e-4, nesterov=True)
optimizer = optim.Adam(net.parameters())
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, 
                                                       threshold=0.001, mode="max") 

def train(epoch, fold):
    print('\nEpoch: %d' % epoch)
    net.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader)):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "train_acc": 1.*correct/total,
        # "lr": scheduler.optimizer.param_groups[0]['lr'],
    })
        
    


def test(epoch, fold):
    global best_acc
    net.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in tqdm(enumerate(test_loader)):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
#                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))

    # Save checkpoint.
    acc = 1.*correct/total
    wandb.log({
        "epoch": epoch,
        "test_loss": test_loss,
        "test_acc": acc,
    })
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net.state_dict(),
            'acc': acc,
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state, f'./checkpoint/hand_{fold}.pth')
        wandb.save(f'./checkpoint/hand_{fold}.pth')
        best_acc = acc
    # scheduler.step(acc)


# In[ ]:


for fold, (train_idx, test_idx) in enumerate(split.split(np.arange(len(datasets)))):
    print(len(train_idx), len(test_idx))
    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)
    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)
    train_loader = torch.utils.data.DataLoader(datasets, num_workers=5, sampler=train_sampler, batch_size=128)
    test_loader = torch.utils.data.DataLoader(datasets, num_workers=5, sampler=test_sampler, batch_size=128)
    lr = 0.1
    best_acc = 0.0
    start_epoch = 0
    

    # optimizer = optim.SGD(net.parameters(), lr=lr,
    #                  momentum=0.9, weight_decay=5e-4, nesterov=True)
    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, 
    #                                                        threshold=0.001, mode="max") 
    
    net = tv.models.resnet18(pretrained=False)
    net.fc = nn.Linear(in_features=512, out_features=5, bias=True)

#     net.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)
#     net.maxpool = nn.Identity()
    net = net.to(device)
    
    
     # optimizer = optim.SGD(net.parameters(), lr=lr,
    #                  momentum=0.9, weight_decay=5e-4, nesterov=True)
    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, 
    #                                                        threshold=0.001, mode="max") 
    optimizer = optim.Adam(net.parameters())
    
    
    wandb.init(
        project="AI Capstone Project 1",
        config = {
            "lr": 1e-4,
            "arch": "mlp_kfold",
            "dataset": "terrain",
            "epochs": 200,
            "fold": fold+1,
        },
    )
    print(f"========== Fold {fold+1} ==========")

    
    for epoch in range(start_epoch, start_epoch+60):
        train(epoch, fold)
        test(epoch, fold)
        print(best_acc)
        
    del train_loader
    del test_loader
        
    wandb.finish()
\end{minted}
\end{code}

\end{appendices}

\end{document}
